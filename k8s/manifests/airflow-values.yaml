executor: "KubernetesExecutor"
webserverSecretKey: webserver-secret-key
webserverSecretKeySecretName: airflow-webserver-secret
data:
  metadataConnection:
    user: user
    pass: password
    db: airflow
dags:
  gitSync:
    enabled: true
    repo: git@github.com:S0rrow/FPT5.git
    branch: dev
    rev: HEAD
    depth: 1
    maxFailures: 3
    subPath: dags
    sshKeySecret: "airflow-ssh-git-secret"
    wait: 60

scheduler:
  livenessProbe:
    initialDelaySeconds: 10
    timeoutSeconds: 120
    failureThreshold: 20
    periodSeconds: 60
  replicas: 2
## add resource part
  resources:
    requests:
      memory: "2Gi"  
      cpu: "500m"    
    limits:
      memory: "4Gi"  
      cpu: "1"       

webserver:
  service:
    type: NodePort
    ports:
       - name: airflow-ui
         port: "{{ .Values.ports.airflowUI }}"
         targetPort: "{{ .Values.ports.airflowUI }}"
         nodePort: 31151
logs:
  persistence:
    enabled: true
    existingClaim: airflow-logs-pvc
    size: 10Gi

workers:
  replicas: 3
## add resource parts
  resources:
    requests:
      memory: "4Gi"
      cpu: "500m"  
    limits:
      memory: "8Gi"
      cpu: "2"

postgresql:
  enabled: true
  image:
    tag: 11.22.0
  auth:
    enablePostgresUser: true
    postgresPassword: postgres
    username: user
    password: password
    database: "airflow"
  primary:
    service:
      type: NodePort
      nodePorts:
        postgresql: 30032
    persistence:
      existingClaim: postgresql-pvc
      size: 20Gi
redis:
  enabled: false
env:
  #  - name: "AIRFLOW__LOGGING_REMOTE_LOGGING"
  #  value: "True"
  #- name: "AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID"
  #  value: "S3Conn"
  #- name: "AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER"
  #  value: "s3://{bucket_name}/airflow/logs"
  #- name: "AIRFLOW__LOGGING__ENCRYPT_S3_LOGS"
  #  value: "False"
  - name: "AIRFLOW__CORE__DEFAULT_TIMEZONE"
    value: "Asia/Seoul"
